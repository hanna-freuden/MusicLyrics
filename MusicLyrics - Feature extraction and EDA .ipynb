{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction and EDA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import libraries and explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import wordninja\n",
    "from itertools import islice\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pd.read_pickle('df_music'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5100 entries, 0 to 5099\n",
      "Data columns (total 29 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Rank              5100 non-null   int64  \n",
      " 1   Song              5100 non-null   object \n",
      " 2   Artist            5100 non-null   object \n",
      " 3   Year              5100 non-null   int64  \n",
      " 4   Lyrics            4913 non-null   object \n",
      " 5   Source            4913 non-null   float64\n",
      " 6   Artists clean     5100 non-null   object \n",
      " 7   artist_song1      5100 non-null   object \n",
      " 8   songs_clean       5100 non-null   object \n",
      " 9   artist_song2      5100 non-null   object \n",
      " 10  danceability      5083 non-null   object \n",
      " 11  energy            5083 non-null   object \n",
      " 12  key               5083 non-null   object \n",
      " 13  loudness          5083 non-null   object \n",
      " 14  mode              5083 non-null   object \n",
      " 15  speechiness       5083 non-null   object \n",
      " 16  acousticness      5083 non-null   object \n",
      " 17  instrumentalness  5083 non-null   object \n",
      " 18  liveness          5083 non-null   object \n",
      " 19  valence           5083 non-null   object \n",
      " 20  tempo             5083 non-null   object \n",
      " 21  type              5083 non-null   object \n",
      " 22  id                5083 non-null   object \n",
      " 23  uri               5083 non-null   object \n",
      " 24  track_href        5083 non-null   object \n",
      " 25  analysis_url      5083 non-null   object \n",
      " 26  duration_ms       5083 non-null   object \n",
      " 27  time_signature    5083 non-null   object \n",
      " 28  error             1 non-null      object \n",
      "dtypes: float64(1), int64(2), object(26)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5100, 29)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Drop songs without or with missing lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of remaining songs is:  4897\n"
     ]
    }
   ],
   "source": [
    "# drop rows with missing values\n",
    "\n",
    "df = df[df.Lyrics != \" NA \"]\n",
    "df.dropna( how='any', subset=['Lyrics'], inplace=True)\n",
    "#reset index\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "print(\"The number of remaining songs is: \", df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of remaining songs is:  4878\n"
     ]
    }
   ],
   "source": [
    "#remove songs that are instrumental\n",
    "\n",
    "df=df[df['Lyrics']!='instrumental'] \n",
    "df=df[df['Lyrics']!=' instrumental'] \n",
    "df=df[df['Lyrics']!=' instrumental '] \n",
    "df=df[df['Lyrics']!='instrumental ']\n",
    "print(\"The number of remaining songs is: \", df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clean lyrics and tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean lyrics - remove punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics=[]\n",
    "\n",
    "for lyric in df.Lyrics: \n",
    "    lyric_string=re.sub('[^A-Za-z]+', ' ', lyric)\n",
    "    lyrics_string = re.sub('/\\s\\s+/g', ' ', lyric)\n",
    "    lyrics.append(lyric_string.lstrip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply wordninja as some lyrics have words crunched together in one string\n",
    "lyrics_clean=[]\n",
    "for lyric in lyrics :\n",
    "    string=wordninja.split(lyric)\n",
    "    title=\"\"\n",
    "    for s in string:\n",
    "        title+=s+\" \" \n",
    "    lyrics_clean.append(title.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add to df\n",
    "df['lyrics_clean']=lyrics_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply lemmatizer and tokenizer\n",
    "# apply word tokenizer, delete stopwords, and apply lemmatizer\n",
    "tokens=[]\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for row in lyrics_clean:\n",
    "    row_tokens=word_tokenize(row)\n",
    "    filtered_sent = [w for w in row_tokens if not w.lower() in stop_words]\n",
    "    stemmed = [lemmatizer.lemmatize(word) for word in filtered_sent]\n",
    "    tokens.append(stemmed)\n",
    "df['tokens']=tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['tokens', 'Rank']].to_pickle('df_tokenized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Make more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4831, 31)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Word frequency features\n",
    "df.unique_words=df.tokens.apply(lambda x: len(set(x) ))\n",
    "df.total_words=df.tokens.apply(lambda x: len(x) )\n",
    "df.word_length=df.tokens.apply(lambda x: np.mean([len(word) for word in x]))\n",
    "\n",
    "#keep only rows where no of words !=0\n",
    "df=df[df.unique_words!=0]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get types of words per song by POS tagging\n",
    "from nltk.tag import pos_tag\n",
    "import nltk \n",
    "#nltk.download('tagsets')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of pos tags per text\n",
    "def get_pos(text):\n",
    "    pos=[]\n",
    "    for i in range(len(text)):\n",
    "        pos.append(pos_tag(text)[i][1])\n",
    "    return dict(pd.Series(pos).value_counts())\n",
    "pos_count=list(df.tokens.apply(get_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate features for word difficulty \n",
    "\n",
    "import syllables\n",
    "import spacy\n",
    "from textstat.textstat import textstatistics, legacy_round\n",
    "\n",
    "def syllables_count(word):\n",
    "    return syllables.estimate(word)\n",
    " \n",
    "# Returns the average number of syllables per word in the text\n",
    "def avg_syllables_per_word(text):\n",
    "    syllable = syllables_count(text)\n",
    "    words = word_count(text)\n",
    "    ASPW = float(syllable) / float(words)\n",
    "    return legacy_round(ASPW, 1)\n",
    "\n",
    "# Return total no of Difficult Words in a text (difficult words are those with syllables >= 2)\n",
    "    # a list of common words)\n",
    "def difficult_words(text):\n",
    "    diff_words_set = set()  \n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    for word in text:\n",
    "        syllable_count = syllables_count(word)\n",
    "        if word not in nlp.Defaults.stop_words and syllable_count >= 2:\n",
    "            diff_words_set.add(word)\n",
    "    return len(diff_words_set)\n",
    " \n",
    "# Count of  polysyllablic words (if word has more than 3 syllables)\n",
    "\n",
    "def poly_syllable_count(text):\n",
    "    count = 0  \n",
    "    for word in text:\n",
    "        syllable_count = syllables_count(word)\n",
    "        if syllable_count >= 3:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find number of words per song that are perfect rhymes \n",
    "#perfect rhymes: DOG -> COG\n",
    "\n",
    "from Phyme import Phyme\n",
    "\n",
    "ph = Phyme()\n",
    "def get_rhymes(word):\n",
    "    list_dict=[value for key,value in ph.get_perfect_rhymes(word).items()]\n",
    "    all_rhymes=[]\n",
    "    for row in list_dict:\n",
    "        for word in row:\n",
    "            all_rhymes.append(word)\n",
    "    all_rhymes=list(set(all_rhymes))\n",
    "    return all_rhymes\n",
    "    \n",
    "def count_rhymes(text):\n",
    "    rhyme_count=0\n",
    "    for word in list(set(text)):\n",
    "        try:\n",
    "            all_rhymes=get_rhymes(word) # get list of all words that rhyme\n",
    "            for x in all_rhymes: # for each of these words, check if it is contained in the lyrics\n",
    "                if x in text:\n",
    "                    rhyme_count+=1\n",
    "                else: \n",
    "                    pass\n",
    "        except:\n",
    "            pass\n",
    "    return rhyme_count        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_text_features(df):\n",
    "    df.polysyl_count=df.tokens.apply(poly_syllable_count)\n",
    "    df.difficult_count=df.tokens.apply(difficult_words)\n",
    "    df.count_rhyming_words=df.tokens.apply(count_rhymes)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parallelize - it seems to take even longer??\n",
    "import random\n",
    "from multiprocessing import  Pool\n",
    "def parallelize_dataframe(df, func, n_cores=4):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"df_add_feat\") # save df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PARALLELIZE - it seems like this takes longer?? HOW\n",
    "addfeat = parallelize_dataframe(df, add_text_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#addfeat = add_features(df_test) # takes about a day to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count=pd.DataFrame(pos_count)\n",
    "df=addfeat.join(df_count, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make plot of no of difficult words by chart ranking\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "df_grouped=df[\"Rank\", \"difficult_count\"].groupby(by=[\"Rank\"]).mean()\n",
    "ax = sns.barplot(x=\"Rank\", y=\"difficult_count\", data=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"df_add_feat\") # save df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
