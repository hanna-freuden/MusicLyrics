{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import libraries and explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import wordninja\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pd.read_pickle('df_music'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5100 entries, 0 to 5099\n",
      "Data columns (total 29 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Rank              5100 non-null   int64  \n",
      " 1   Song              5100 non-null   object \n",
      " 2   Artist            5100 non-null   object \n",
      " 3   Year              5100 non-null   int64  \n",
      " 4   Lyrics            4913 non-null   object \n",
      " 5   Source            4913 non-null   float64\n",
      " 6   Artists clean     5100 non-null   object \n",
      " 7   artist_song1      5100 non-null   object \n",
      " 8   songs_clean       5100 non-null   object \n",
      " 9   artist_song2      5100 non-null   object \n",
      " 10  danceability      5083 non-null   object \n",
      " 11  energy            5083 non-null   object \n",
      " 12  key               5083 non-null   object \n",
      " 13  loudness          5083 non-null   object \n",
      " 14  mode              5083 non-null   object \n",
      " 15  speechiness       5083 non-null   object \n",
      " 16  acousticness      5083 non-null   object \n",
      " 17  instrumentalness  5083 non-null   object \n",
      " 18  liveness          5083 non-null   object \n",
      " 19  valence           5083 non-null   object \n",
      " 20  tempo             5083 non-null   object \n",
      " 21  type              5083 non-null   object \n",
      " 22  id                5083 non-null   object \n",
      " 23  uri               5083 non-null   object \n",
      " 24  track_href        5083 non-null   object \n",
      " 25  analysis_url      5083 non-null   object \n",
      " 26  duration_ms       5083 non-null   object \n",
      " 27  time_signature    5083 non-null   object \n",
      " 28  error             1 non-null      object \n",
      "dtypes: float64(1), int64(2), object(26)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5100, 29)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Drop songs without or with missing lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of remaining songs is:  4897\n"
     ]
    }
   ],
   "source": [
    "# drop rows with missing values\n",
    "\n",
    "df = df[df.Lyrics != \" NA \"]\n",
    "df.dropna( how='any', subset=['Lyrics'], inplace=True)\n",
    "#reset index\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "print(\"The number of remaining songs is: \", df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of remaining songs is:  4878\n"
     ]
    }
   ],
   "source": [
    "#remove songs that are instrumental\n",
    "\n",
    "df=df[df['Lyrics']!='instrumental'] \n",
    "df=df[df['Lyrics']!=' instrumental'] \n",
    "df=df[df['Lyrics']!=' instrumental '] \n",
    "df=df[df['Lyrics']!='instrumental ']\n",
    "print(\"The number of remaining songs is: \", df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clean lyrics and tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean lyrics - remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics=[]\n",
    "\n",
    "for lyric in df.Lyrics: \n",
    "    lyric_string=re.sub('[^A-Za-z]+', ' ', lyric)\n",
    "    lyrics_string = re.sub('/\\s\\s+/g', ' ', lyric)\n",
    "    lyrics.append(lyric_string.lstrip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply wordninja as some lyrics have words crunched together in one string\n",
    "lyrics_clean=[]\n",
    "for lyric in lyrics :\n",
    "    string=wordninja.split(lyric)\n",
    "    title=\"\"\n",
    "    for s in string:\n",
    "        title+=s+\" \" \n",
    "    lyrics_clean.append(title.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add to df\n",
    "df['lyrics_clean']=lyrics_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply lemmatizer and tokenizer\n",
    "# apply word tokenizer, delete stopwords, and apply lemmatizer\n",
    "tokens=[]\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for row in lyrics_clean:\n",
    "    row_tokens=word_tokenize(row)\n",
    "    filtered_sent = [w for w in row_tokens if not w.lower() in stop_words]\n",
    "    stemmed = [lemmatizer.lemmatize(word) for word in filtered_sent]\n",
    "    tokens.append(stemmed)\n",
    "df['tokens']=tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Make more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4831"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# more features\n",
    "\n",
    "#get no of unique words per song lyric\n",
    "count_words=[]\n",
    "for row in df.tokens:\n",
    "    unique = set(row) \n",
    "    count_words.append(len(unique))\n",
    "#and add to df\n",
    "df[\"unique_words\"]=count_words\n",
    "\n",
    "#keep only rows where no of words !=0\n",
    "df=df[df['unique_words']!=0]\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "##get total no of words per song lyric\n",
    "total_words=[]\n",
    "for row in df.tokens:\n",
    "    length = len(row) \n",
    "    total_words.append(length)\n",
    "df['total_words']=total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get avg word length\n",
    "word_length=[]\n",
    "for row in df.tokens:\n",
    "    length_row = len(row)\n",
    "    length_word=sum([len(word) for word in row])/length_row\n",
    "    word_length.append(length_word)\n",
    "df['word_length']=word_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rhyming words\n",
    "\n",
    "#!pip install phyme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Phyme import Phyme\n",
    "\n",
    "ph = Phyme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find number of words per song that are perfect rhymes \n",
    "#perfect rhymes. DOG -> COG\n",
    "def get_rhymes(word):\n",
    "    list_dict=[value for key,value in ph.get_perfect_rhymes(word).items()]\n",
    "    all_rhymes=[]\n",
    "    for row in list_dict:\n",
    "        for word in row:\n",
    "            all_rhymes.append(word)\n",
    "    all_rhymes=list(set(all_rhymes))\n",
    "    return all_rhymes\n",
    "    \n",
    "def count_rhymes(text):\n",
    "    rhyme_count=0\n",
    "    for word in list(set(text)):\n",
    "        try:\n",
    "            all_rhymes=get_rhymes(word) # get list of all words that rhyme\n",
    "            for x in all_rhymes: # for each of these words, check if it is contained in the lyrics\n",
    "                if x in text:\n",
    "                    rhyme_count+=1\n",
    "                else: \n",
    "                    pass\n",
    "        except:\n",
    "            pass\n",
    "    return rhyme_count          \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply functions to lists of lyric tokens\n",
    "count_rhyming_words=[]\n",
    "for lyric in df.tokens:\n",
    "    count=count_rhymes(lyric)\n",
    "    count_rhyming_words.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['count_rhyming_words']=count_rhyming_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply tfidf vectorizer after count vecotizer with n_gram_range==5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1862379"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def dummy(tokens):\n",
    "    return tokens\n",
    "\n",
    "cv = CountVectorizer(\n",
    "        tokenizer=dummy,\n",
    "        preprocessor=dummy,\n",
    "        ngram_range=(1, 5)\n",
    "    )  \n",
    "tokens=df.tokens\n",
    "x = cv.fit_transform(tokens)\n",
    "words = cv.get_feature_names()\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply tfidf vectorizer \n",
    "# Create the tf-idf representation using the bag-of-words matrix\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transform = TfidfTransformer(norm=None)\n",
    "\n",
    "X_tfidf = tfidf_transform.fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Join music features, lyrics features and tfidf features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_features=['danceability', 'key', 'loudness','energy','acousticness', 'speechiness', 'mode', 'instrumentalness', 'liveness','valence', 'tempo']\n",
    "X_music=df[music_features]\n",
    "X_music=X_music.fillna(X_music.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of lyrics features\n",
    "#also include year as a feature\n",
    "add_features=['word_length', 'unique_words', 'total_words','count_rhyming_words', 'Year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4831, 11)\n",
      "(4831, 5)\n",
      "(4831, 1862379)\n",
      "(4831, 1862395)\n"
     ]
    }
   ],
   "source": [
    "#join lyrics data with musical features\n",
    "#stack sparse matrices\n",
    "from scipy.sparse import hstack\n",
    "import scipy.sparse\n",
    "X_m=scipy.sparse.csr_matrix(X_music.values)\n",
    "print(X_m.shape)\n",
    "\n",
    "X_l=scipy.sparse.csr_matrix(df[add_features].values)\n",
    "print(X_l.shape)\n",
    "\n",
    "print(X_tfidf.shape)\n",
    "X=hstack((X_m, X_tfidf, X_l))\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4831"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save labels: ranking<50=1, 0 otherwise\n",
    "y=pd.cut(df.Rank,bins=[0,50,100],labels=[1,0])\n",
    "df['y']=y\n",
    "df[['y','Rank']].groupby('y').mean()\n",
    "len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make test_train_split\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate Logistic Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def simple_logistic_classify(X_train, y_train, X_test, y_test, _C=1.0):\n",
    "    model = LogisticRegression(C=_C,  max_iter=10000).fit(X_train, y_train)\n",
    "    score_test=roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "    #get most important features\n",
    "    #importance =dict(zip(X_train.columns, list(model.coef_[0])))\n",
    "    #sort={k: v for k, v in sorted(importance.items(), reverse=True, key=lambda item: item[1])}\n",
    "   # n_items=list(islice(sort.items(), 5))\n",
    "\n",
    "    print('ROC-AUC Score Test', score_test)\n",
    "    #print('5 most important items', n_items)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def simple_rf_classify(X_train, y_train, X_test, y_test):\n",
    "    model =  RandomForestClassifier(max_depth=2, random_state=0).fit(X_train, y_train)\n",
    "    score_test=roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "    #get most important features\n",
    "    #importance =dict(zip(X_train.columns, list(model.coef_[0])))\n",
    "    #sort={k: v for k, v in sorted(importance.items(), reverse=True, key=lambda item: item[1])}\n",
    "    #n_items=list(islice(sort.items(), 5))\n",
    "    print('ROC-AUC Score Test', score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple logistic model of music lyrics word vectors, lyrics features and musical features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC Score Test 0.49145978605824425\n"
     ]
    }
   ],
   "source": [
    "simple_logistic_classify(X_train, y_train, X_test, y_test, _C=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC Score Test 0.5227214825578653\n"
     ]
    }
   ],
   "source": [
    "simple_rf_classify(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define parameter grid for logistic cv\n",
    "\n",
    "C = np.logspace(-4, 4, 50)\n",
    "penalty = ['l2']\n",
    "\n",
    "parameters = dict(C=C, penalty=penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_cv(X,y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.33, random_state=42)\n",
    "    logistic=LogisticRegression(max_iter=500)\n",
    "    gsl=GridSearchCV(logistic, parameters, cv=3, n_jobs=-1, scoring=\"roc_auc\")\n",
    "    gsl.fit(X_train, y_train)\n",
    "    \n",
    "    return gsl\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Classifier\n",
    "lr=logistic_cv(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit tuned model\n",
    "    \n",
    "print(\"tuned hyperparameters :(best parameters) \", gsl.best_params_)\n",
    "print(\"ROC AUC :\", gsl.best_params_)\n",
    "\n",
    "logisticCV=LogisticRegression(C=gsl.best_params_['C'], penalty=gsl.best_params_['penalty'], max_iter=500)\n",
    "logisticCV.fit(X_train, y_train)\n",
    "\n",
    "  \n",
    "#return predicted probabilities\n",
    "lr_probs = logisticCV.predict_proba(X_test)\n",
    "\n",
    "    # keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:, 1]\n",
    "roc_auc=roc_auc_score(y_test, lr_probs)\n",
    "\n",
    "    # get tpr, fpr\n",
    "fpr, tpr, _ = roc_curve(y_test, lr_probs)\n",
    "\n",
    "\n",
    "print(\"Tuned random logit  model \", roc_auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest model\n",
    "#define parameter grid for randomized search with forest model\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#n_estimators\n",
    "n_estimators = [250,300, 350]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap,\n",
    "               'n_estimators':n_estimators}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_rs(X,y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.33, random_state=42)\n",
    "    #Instantiate the classifier\n",
    "    rf=RandomForestClassifier(n_jobs=-1)\n",
    "    rs=RandomizedSearchCV(rf,random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1, scoring='roc_auc')\n",
    "    rs.fit(X_train, y_train)\n",
    "    \n",
    "    return rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random forest classifier\n",
    "rf=rf_rs(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit tuned model\n",
    "print(\"tuned hyperparameters :(best parameters) \", rs.best_params_)\n",
    "print(\"ROC AUC :\", rs.best_params_)\n",
    "\n",
    "\n",
    "rfCV=RandomForestClassifier(max_features=rf.best_params_['max_features'], max_depth=rf.best_params_['max_depth'], min_samples_split=rf.best_params_['min_samples_split'], min_samples_leaf=rf.best_params_['min_samples_leaf'], bootstrap=rf.best_params_['bootstrap'],n_estimators=rf.best_params_['n_estimators'])\n",
    "rfCV.fit(X_train, y_train)\n",
    "\n",
    "#y_pred=logisticCV.predict(X_test)\n",
    "    \n",
    "#return predicted probabilities\n",
    "probs = rfCV.predict_proba(X_test)\n",
    "    # keep probabilities for the positive outcome only\n",
    "probs = probs[:, 1]\n",
    "roc_auc=roc_auc_score(y_test, probs)\n",
    "\n",
    "    # get tpr, fpr\n",
    "fpr, tpr, _ = roc_curve(y_test, probs)\n",
    "    \n",
    "print(\"Tuned random forest model \", roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drawing roc curve and auc\n",
    "\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# generate a no skill prediction (majority class)\n",
    "ns_probs = [0 for _ in range(len(y_test))]\n",
    "\n",
    "## plot the roc curve: start with no skill prediction\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "ns_auc = roc_auc_score(y_test, ns_probs) #no skill\n",
    "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
    "plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "\n",
    "\n",
    "#draw roc curves for tuned models\n",
    "def draw_roc_curve(model, label):\n",
    "\n",
    "    #get predicted probabilities\n",
    "    prob=list(model[1])\n",
    "    # calculate roc auc scores\n",
    "    \n",
    "    auc = roc_auc_score(y_test, prob)\n",
    "    # calculate roc curves\n",
    "    fpr=model[2]\n",
    "    tpr=model[3]\n",
    "    print('Model=%s: ROC AUC=%.3f' % (label, auc)) \n",
    "    plt.plot(fpr, tpr, marker='.', label=label)\n",
    "\n",
    "draw_roc_curve(lr, 'Logistic regression, tuned')\n",
    "draw_roc_curve(rf, 'Random forest, tuned')\n",
    "\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
