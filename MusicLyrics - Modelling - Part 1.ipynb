{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling - Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import wordninja\n",
    "from itertools import islice\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_pickle(\"df_add_feat\") # reload df from pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. TFIDF-Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply tfidf vectorizer after count vecotizer with n_gram_range==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def dummy(tokens):\n",
    "    return tokens\n",
    "\n",
    "cv = CountVectorizer(\n",
    "        tokenizer=dummy,\n",
    "        preprocessor=dummy,\n",
    "        ngram_range=(1, 1)\n",
    "    )  \n",
    "tokens=df.tokens\n",
    "x = cv.fit_transform(tokens)\n",
    "words = cv.get_feature_names()\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf vectorizer \n",
    "vectorizer = TfidfVectorizer(tokenizer=dummy,\n",
    "        preprocessor=dummy,\n",
    "        ngram_range=(1, 1))\n",
    "\n",
    "X_tfidf = vectorizer.fit(tokens.values) \n",
    "\n",
    "idf_scores = X_tfidf.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make histogram of idf scores\n",
    "maxi=np.max(idf_scores)\n",
    "mean=np.mean(idf_scores)\n",
    "min=np.min(idf_scores)\n",
    "print(maxi, mean, min)\n",
    "len(idf_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.histplot(data=idf_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf.get_feature_names()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out ngrams that occur in too few or too many songs\n",
    "\n",
    "filtered_indices = np.argwhere(((idf_scores>6) ))\n",
    "filtered_indices = [idx[0] for idx in filtered_indices]\n",
    "\n",
    "#list of vocabulary from the vectorizer\n",
    "vocabulary = X_tfidf.get_feature_names()\n",
    "\n",
    "#preparing a list with filtered vocabulary\n",
    "filtered_voc = [vocabulary[i] for i in filtered_indices]\n",
    "\n",
    "#size before and after filtering\n",
    "print(\"no of words before filtering: \", len(idf_scores))\n",
    "print(\"no of words after filtering: \", len(filtered_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_voc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep only filtered indices\n",
    "X_tfidf=vectorizer.fit_transform(tokens.values)[:,filtered_indices]\n",
    "type(X_tfidf)\n",
    "print(X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tf-idf feature selection using tuned random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_pickle(\"df_add_feat\") # reload df from pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest model\n",
    "#define parameter grid for randomized search with forest model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#n_estimators\n",
    "n_estimators = [250,300, 350]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap,\n",
    "               'n_estimators':n_estimators}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_rs(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    #Instantiate the classifier\n",
    "    rf=RandomForestClassifier(n_jobs=-1)\n",
    "    rs=RandomizedSearchCV(rf,random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1, scoring='roc_auc')\n",
    "    rs.fit(X_train, y_train)\n",
    "    return rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit tuned model\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "def fit_tuned_rf_model(model):\n",
    "    print(\"tuned hyperparameters :(best parameters) \", model.best_params_)\n",
    "    print(\"ROC AUC :\", model.best_score_)\n",
    "\n",
    "    modelCV=RandomForestClassifier(max_features=model.best_params_['max_features'], max_depth=model.best_params_['max_depth'], min_samples_split=model.best_params_['min_samples_split'], min_samples_leaf=model.best_params_['min_samples_leaf'], bootstrap=model.best_params_['bootstrap'],n_estimators=model.best_params_['n_estimators'])\n",
    "    modelCV.fit(X_train, y_train)\n",
    "\n",
    "    #return predicted probabilities\n",
    "    probs = modelCV.predict_proba(X_test)\n",
    "        # keep probabilities for the positive outcome only\n",
    "    probs = probs[:, 1]\n",
    "    roc_auc=roc_auc_score(y_test, probs)\n",
    "    fpr, tpr, _ = roc_curve(y_test, probs)\n",
    "    print(\"Fitted tuned random forest model \", roc_auc)\n",
    "    return modelCV, roc_auc, fpr, tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save labels: ranking<50=1, 0 otherwise\n",
    "y=pd.cut(df.Rank,bins=[0,50,100],labels=[1,0])\n",
    "df['y']=y\n",
    "df[['y','Rank']].groupby('y').mean()\n",
    "len(y)\n",
    "df.Rank.groupby(y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random forest classifier on tfidf features\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf,y, test_size=0.33, random_state=42)\n",
    "rf=rf_rs(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the tuned model to disk\n",
    "filename = 'tuned_rf_model.sav'\n",
    "pickle.dump(rf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit tuned model\n",
    "rf_fit=fit_tuned_rf_model(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot most important features\n",
    "importance=rf_fit[0].feature_importances_\n",
    "feat_dict=dict(zip(filtered_voc,importance))\n",
    "sort={k: v for k, v in sorted(feat_dict.items(), reverse=True, key=lambda item: item[1])}\n",
    "n_items=list(islice(sort.items(), 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs=[round(i[1],5) for i in n_items]\n",
    "indices=[i[0] for i in n_items]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "plt.barh(indices, coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select top 1000 features\n",
    "top_1000=list(sort.items())[:1000]\n",
    "top_1000_feat=[i[0] for i in top_1000]\n",
    "#select top feat from X_tfidf\n",
    "#get indices of top1000 features from filtered_voc\n",
    "top_1000_indices = [i for i, value in enumerate(filtered_voc) if value in top_1000_feat]\n",
    "X_1000=X_tfidf[:,top_1000_indices]\n",
    "X_1000.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Join music features, lyrics features and tfidf features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_features=['danceability', 'key', 'loudness','energy','acousticness', 'speechiness', 'mode', 'instrumentalness', 'liveness','valence', 'tempo']\n",
    "X_music=df[music_features]\n",
    "X_music=X_music.fillna(X_music.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of lyrics features\n",
    "\n",
    "add_features=['word_length', 'unique_words', 'total_words','count_rhyming_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join lyrics data with musical features\n",
    "#stack sparse matrices\n",
    "from scipy.sparse import hstack\n",
    "import scipy.sparse\n",
    "\n",
    "X_m=scipy.sparse.csr_matrix(X_music.values)\n",
    "print(X_m.shape)\n",
    "\n",
    "X_l=scipy.sparse.csr_matrix(df[add_features].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_1000.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#array of column names\n",
    "columns=filtered_voc+music_features+add_features\n",
    "col_names = np.array(columns)\n",
    "col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text=hstack([X_1000, X_l])\n",
    "X_all=hstack([X_text,X_m])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define parameter grid for logistic cv\n",
    "\n",
    "C = [0.00001,0.0001, 0.001, 0.01, 0.1,1,10,100,1000, 10000]\n",
    "penalty = ['l2']\n",
    "\n",
    "parameters = dict(C=C, penalty=penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_cv(X_train, X_test, y_train, y_test):\n",
    "    logistic=LogisticRegression(max_iter=500)\n",
    "    gsl=GridSearchCV(logistic, parameters, cv=3, n_jobs=-1, scoring=\"roc_auc\")\n",
    "    gsl.fit(X_train, y_train)\n",
    "    return gsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit tuned model\n",
    "def fit_tuned_lr_model(model):    \n",
    "    logisticCV=LogisticRegression(C=model.best_params_['C'], penalty=model.best_params_['penalty'], max_iter=500)\n",
    "    logisticCV.fit(X_train, y_train)\n",
    "\n",
    "    #return predicted probabilities\n",
    "    lr_probs = logisticCV.predict_proba(X_test)\n",
    "\n",
    "    # keep probabilities for the positive outcome only\n",
    "    lr_probs = lr_probs[:, 1]\n",
    "    roc_auc=roc_auc_score(y_test, lr_probs)\n",
    "    fpr, tpr, _ = roc_curve(y_test, lr_probs)\n",
    "    print(\"tuned hyperparameters :(best parameters) \", model.best_params_)\n",
    "    print(\"ROC AUC :\", model.best_score_)\n",
    "    print(\"Fitted tuned logit  model \", roc_auc)\n",
    "    return logisticCV,roc_auc, fpr, tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Classifier on tfidf features \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf,y, test_size=0.33, random_state=42)\n",
    "lr=logistic_cv(X_train, X_test, y_train, y_test)\n",
    "#fit tuned model\n",
    "lr_fit=fit_tuned_lr_model(lr)\n",
    "# save the tuned model to disk\n",
    "filename = 'tuned_logistic_tfidf.sav'\n",
    "pickle.dump(lr, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Classifier on tfidf top 1000 features only \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_1000,y, test_size=0.33, random_state=42)\n",
    "lr=logistic_cv(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#fit tuned model\n",
    "lr_fit_1000=fit_tuned_lr_model(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the tuned model to disk\n",
    "filename = 'tuned_logistic_1000.sav'\n",
    "pickle.dump(lr, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Classifier on tfidf top 1000 plus engineered text features  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_text,y, test_size=0.33, random_state=42)\n",
    "lr=logistic_cv(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the tuned model to disk\n",
    "filename = 'tuned_logistic_text.sav'\n",
    "pickle.dump(lr, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit tuned model\n",
    "lr_fit_text=fit_tuned_lr_model(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Classifier on tfidf top 1000 plus engineered text features plus music features\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all,y, test_size=0.33, random_state=42)\n",
    "lr=logistic_cv(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit tuned model\n",
    "lr_fit_all=fit_tuned_lr_model(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random forest classifier on tfidf and text features\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_text,y, test_size=0.33, random_state=42)\n",
    "rf=rf_rs(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the tuned model to disk\n",
    "filename = 'tuned_rf_only_text.sav'\n",
    "pickle.dump(rf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit tuned model\n",
    "rf_fit_text=fit_tuned_rf_model(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random forest classifier on all 1000 tfidf plus text and musical features\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all,y, test_size=0.33, random_state=42)\n",
    "rf=rf_rs(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the tuned model to disk\n",
    "filename = 'tuned_rf_all.sav'\n",
    "pickle.dump(rf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit tuned model\n",
    "rf_fit_all=fit_tuned_rf_model(rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#support vector mechines (SVM)\n",
    "from sklearn.svm import SVC\n",
    "#initiate model\n",
    "svm = SVC(probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tune hyperparameters with GridSearchCV\n",
    "# defining parameter range\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all,y, test_size=0.33, random_state=42)\n",
    "\n",
    "param_grid = {'C': [0.1, 1, 10, 100, 1000], \n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "              'kernel': ['rbf']} \n",
    "  \n",
    "svm_cv = GridSearchCV(svm, param_grid, cv=3, n_jobs=-1, scoring=\"roc_auc\")\n",
    "  \n",
    "# fitting the model for grid search\n",
    "svm_cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Tuned SVM Params: {}\".format(svm_cv.best_params_)) \n",
    "print(\"Best score is {}\".format(svm_cv.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make model with tuned parameters\n",
    "svm_tuned=SVC(C=svm_cv.best_params_['C'], gamma= svm_cv.best_params_['gamma'], kernel= svm_cv.best_params_['kernel'], probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_probs = svm_tuned.predict_proba(X_test)\n",
    "\n",
    "# keep probabilities for the positive outcome only\n",
    "svm_probs = svm_probs[:, 1]\n",
    "roc_auc=roc_auc_score(y_test, svm_probs)\n",
    "fpr, tpr, _ = roc_curve(y_test, svm_probs)\n",
    "\n",
    "print(\"Fitted tuned svm model \", roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drawing roc curve and auc\n",
    "\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# generate a no skill prediction (majority class)\n",
    "ns_probs = [0 for _ in range(len(y_test))]\n",
    "\n",
    "## plot the roc curve: start with no skill prediction\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "ns_auc = roc_auc_score(y_test, ns_probs) #no skill\n",
    "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
    "plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "\n",
    "\n",
    "#draw roc curves for tuned models\n",
    "def draw_roc_curve(model, label):\n",
    "    model_name=type(model[0]).__name__\n",
    "    # retrieve roc auc scores\n",
    "    auc = model[1]\n",
    "    # calculate roc curves\n",
    "    # get tpr, fpr\n",
    "    fpr=model[2]\n",
    "    tpr=model[3]\n",
    "    print('Model=%s; Data=%s; ROC AUC=%.3f' % (model_name, label, auc)) \n",
    "    lab=model_name+\" \"+label\n",
    "    plt.plot(fpr, tpr, marker='.', label=lab)\n",
    "\n",
    "    \n",
    "model_dict={'all tfidf features':lr_fit_tfidf, 'top 1000 tfidf features': lr_fit_1000, 'top tfidf plus text features': lr_fit_text,'all features': lr_fit_all, 'top 1000 tfidf features': rf_fit, 'top tfidf plus text features': rf_fit_text,'all features': rf_fit_all}\n",
    "\n",
    "\n",
    "#WHY CAN I NOT DRAW ALL LINES IN ONE GRAPH USING A LOOP?\n",
    "for k,v in model_dict.items():\n",
    "    draw_roc_curve(v,k)\n",
    "\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
