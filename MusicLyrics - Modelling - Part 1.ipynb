{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import libraries and explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import wordninja\n",
    "from itertools import islice\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pd.read_pickle('df_music'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5100 entries, 0 to 5099\n",
      "Data columns (total 29 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Rank              5100 non-null   int64  \n",
      " 1   Song              5100 non-null   object \n",
      " 2   Artist            5100 non-null   object \n",
      " 3   Year              5100 non-null   int64  \n",
      " 4   Lyrics            4913 non-null   object \n",
      " 5   Source            4913 non-null   float64\n",
      " 6   Artists clean     5100 non-null   object \n",
      " 7   artist_song1      5100 non-null   object \n",
      " 8   songs_clean       5100 non-null   object \n",
      " 9   artist_song2      5100 non-null   object \n",
      " 10  danceability      5083 non-null   object \n",
      " 11  energy            5083 non-null   object \n",
      " 12  key               5083 non-null   object \n",
      " 13  loudness          5083 non-null   object \n",
      " 14  mode              5083 non-null   object \n",
      " 15  speechiness       5083 non-null   object \n",
      " 16  acousticness      5083 non-null   object \n",
      " 17  instrumentalness  5083 non-null   object \n",
      " 18  liveness          5083 non-null   object \n",
      " 19  valence           5083 non-null   object \n",
      " 20  tempo             5083 non-null   object \n",
      " 21  type              5083 non-null   object \n",
      " 22  id                5083 non-null   object \n",
      " 23  uri               5083 non-null   object \n",
      " 24  track_href        5083 non-null   object \n",
      " 25  analysis_url      5083 non-null   object \n",
      " 26  duration_ms       5083 non-null   object \n",
      " 27  time_signature    5083 non-null   object \n",
      " 28  error             1 non-null      object \n",
      "dtypes: float64(1), int64(2), object(26)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5100, 29)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Drop songs without or with missing lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of remaining songs is:  4897\n"
     ]
    }
   ],
   "source": [
    "# drop rows with missing values\n",
    "\n",
    "df = df[df.Lyrics != \" NA \"]\n",
    "df.dropna( how='any', subset=['Lyrics'], inplace=True)\n",
    "#reset index\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "print(\"The number of remaining songs is: \", df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of remaining songs is:  4878\n"
     ]
    }
   ],
   "source": [
    "#remove songs that are instrumental\n",
    "\n",
    "df=df[df['Lyrics']!='instrumental'] \n",
    "df=df[df['Lyrics']!=' instrumental'] \n",
    "df=df[df['Lyrics']!=' instrumental '] \n",
    "df=df[df['Lyrics']!='instrumental ']\n",
    "print(\"The number of remaining songs is: \", df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clean lyrics and tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean lyrics - remove punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics=[]\n",
    "\n",
    "for lyric in df.Lyrics: \n",
    "    lyric_string=re.sub('[^A-Za-z]+', ' ', lyric)\n",
    "    lyrics_string = re.sub('/\\s\\s+/g', ' ', lyric)\n",
    "    lyrics.append(lyric_string.lstrip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply wordninja as some lyrics have words crunched together in one string\n",
    "lyrics_clean=[]\n",
    "for lyric in lyrics :\n",
    "    string=wordninja.split(lyric)\n",
    "    title=\"\"\n",
    "    for s in string:\n",
    "        title+=s+\" \" \n",
    "    lyrics_clean.append(title.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add to df\n",
    "df['lyrics_clean']=lyrics_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply lemmatizer and tokenizer\n",
    "# apply word tokenizer, delete stopwords, and apply lemmatizer\n",
    "tokens=[]\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for row in lyrics_clean:\n",
    "    row_tokens=word_tokenize(row)\n",
    "    filtered_sent = [w for w in row_tokens if not w.lower() in stop_words]\n",
    "    stemmed = [lemmatizer.lemmatize(word) for word in filtered_sent]\n",
    "    tokens.append(stemmed)\n",
    "df['tokens']=tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['tokens', 'Rank']].to_pickle('df_tokenized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Make more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\freudenreich\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get types of words per song by POS tagging\n",
    "from nltk.tag import pos_tag\n",
    "import nltk \n",
    "nltk.download('tagsets')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include features for word difficulty \n",
    "#!pip install syllables\n",
    "import syllables\n",
    "\n",
    "import spacy\n",
    "\n",
    "from textstat.textstat import textstatistics, legacy_round\n",
    "\n",
    "def syllables_count(word):\n",
    "    return syllables.estimate(word)\n",
    " \n",
    "# Returns the average number of syllables per word in the text\n",
    "def avg_syllables_per_word(text):\n",
    "    syllable = syllables_count(text)\n",
    "    words = word_count(text)\n",
    "    ASPW = float(syllable) / float(words)\n",
    "    return legacy_round(ASPW, 1)\n",
    "\n",
    "# Return total no of Difficult Words in a text (difficult words are those with syllables >= 2)\n",
    "    # a list of common words)\n",
    "def difficult_words(text):\n",
    "    diff_words_set = set()  \n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    #doc = nlp(text)\n",
    "    \n",
    "    for word in text:\n",
    "        syllable_count = syllables_count(word)\n",
    "        if word not in nlp.Defaults.stop_words and syllable_count >= 2:\n",
    "            diff_words_set.add(word)\n",
    " \n",
    "    return len(diff_words_set)\n",
    " \n",
    "# Count of  polysyllablic words (if word has more than 3 syllables)\n",
    "\n",
    "def poly_syllable_count(text):\n",
    "    count = 0\n",
    "  \n",
    "    for word in text:\n",
    "        syllable_count = syllables_count(word)\n",
    "        if syllable_count >= 3:\n",
    "            count += 1\n",
    "    return count\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of pos tags per text\n",
    "def get_pos(text):\n",
    "    pos=[]\n",
    "    for i in range(len(text)):\n",
    "        pos.append(pos_tag(text)[i][1])\n",
    "    return dict(pd.Series(pos).value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    df.polysyl_count=df.lyrics_clean.apply(poly_syllable_count)\n",
    "    df.difficult_count=df.lyrics_clean.apply(difficult_words)\n",
    "    pos_count=list(df.tokens.apply(get_pos))\n",
    "    df_count=pd.DataFrame(pos_count)\n",
    "    df=df.join(df_count, how='inner')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parallelize\n",
    "import random\n",
    "from multiprocessing import  Pool\n",
    "def parallelize_dataframe(df, func, n_cores=5):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "df = add_features(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PARALLELIZE TAKES LONGER ??\n",
    "#df = parallelize_dataframe(df, add_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make plot of no of difficult words by chart ranking\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "df_grouped=df[\"Rank\", \"difficult_count\"].groupby(by=[\"Rank\"]).mean()\n",
    "ax = sns.barplot(x=\"Rank\", y=\"difficult_count\", data=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more features\n",
    "\n",
    "#get no of unique words per song lyric\n",
    "count_words=[]\n",
    "for row in df.tokens:\n",
    "    unique = set(row) \n",
    "    count_words.append(len(unique))\n",
    "#and add to df\n",
    "df[\"unique_words\"]=count_words\n",
    "\n",
    "#keep only rows where no of words !=0\n",
    "df=df[df['unique_words']!=0]\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##get total no of words per song lyric\n",
    "total_words=[]\n",
    "for row in df.tokens:\n",
    "    length = len(row) \n",
    "    total_words.append(length)\n",
    "df['total_words']=total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get avg word length\n",
    "word_length=[]\n",
    "for row in df.tokens:\n",
    "    length_row = len(row)\n",
    "    length_word=sum([len(word) for word in row])/length_row\n",
    "    word_length.append(length_word)\n",
    "df['word_length']=word_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rhyming words\n",
    "\n",
    "#!pip install phyme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Phyme import Phyme\n",
    "\n",
    "ph = Phyme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find number of words per song that are perfect rhymes \n",
    "#perfect rhymes. DOG -> COG\n",
    "def get_rhymes(word):\n",
    "    list_dict=[value for key,value in ph.get_perfect_rhymes(word).items()]\n",
    "    all_rhymes=[]\n",
    "    for row in list_dict:\n",
    "        for word in row:\n",
    "            all_rhymes.append(word)\n",
    "    all_rhymes=list(set(all_rhymes))\n",
    "    return all_rhymes\n",
    "    \n",
    "def count_rhymes(text):\n",
    "    rhyme_count=0\n",
    "    for word in list(set(text)):\n",
    "        try:\n",
    "            all_rhymes=get_rhymes(word) # get list of all words that rhyme\n",
    "            for x in all_rhymes: # for each of these words, check if it is contained in the lyrics\n",
    "                if x in text:\n",
    "                    rhyme_count+=1\n",
    "                else: \n",
    "                    pass\n",
    "        except:\n",
    "            pass\n",
    "    return rhyme_count          \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply functions to lists of lyric tokens\n",
    "count_rhyming_words=[]\n",
    "for lyric in df.tokens:\n",
    "    count=count_rhymes(lyric)\n",
    "    count_rhyming_words.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['count_rhyming_words']=count_rhyming_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply tfidf vectorizer after count vecotizer with n_gram_range==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def dummy(tokens):\n",
    "    return tokens\n",
    "\n",
    "cv = CountVectorizer(\n",
    "        tokenizer=dummy,\n",
    "        preprocessor=dummy,\n",
    "        ngram_range=(1, 1)\n",
    "    )  \n",
    "tokens=df.tokens\n",
    "x = cv.fit_transform(tokens)\n",
    "words = cv.get_feature_names()\n",
    "len(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf vectorizer \n",
    "vectorizer = TfidfVectorizer(tokenizer=dummy,\n",
    "        preprocessor=dummy,\n",
    "        ngram_range=(1, 1))\n",
    "\n",
    "X_tfidf = vectorizer.fit(tokens.values) \n",
    "\n",
    "idf_scores = X_tfidf.idf_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make histogram of idf scores\n",
    "maxi=np.max(idf_scores)\n",
    "mean=np.mean(idf_scores)\n",
    "min=np.min(idf_scores)\n",
    "print(maxi, mean, min)\n",
    "len(idf_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.histplot(data=idf_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf.get_feature_names()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out ngrams that occur in too few or too many songs\n",
    "\n",
    "filtered_indices = np.argwhere(((idf_scores>6) ))\n",
    "filtered_indices = [idx[0] for idx in filtered_indices]\n",
    "\n",
    "#list of vocabulary from the vectorizer\n",
    "vocabulary = X_tfidf.get_feature_names()\n",
    "\n",
    "#preparing a list with filtered vocabulary\n",
    "filtered_voc = [vocabulary[i] for i in filtered_indices]\n",
    "\n",
    "#size before and after filtering\n",
    "print(\"no of words before filtering: \", len(idf_scores))\n",
    "print(\"no of words after filtering: \", len(filtered_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_voc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep only filtered indices\n",
    "X_tfidf=vectorizer.fit_transform(tokens.values)[:,filtered_indices]\n",
    "type(X_tfidf)\n",
    "print(X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tf-idf feature selection using tuned random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest model\n",
    "#define parameter grid for randomized search with forest model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#n_estimators\n",
    "n_estimators = [250,300, 350]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap,\n",
    "               'n_estimators':n_estimators}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_rs(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    #Instantiate the classifier\n",
    "    rf=RandomForestClassifier(n_jobs=-1)\n",
    "    rs=RandomizedSearchCV(rf,random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1, scoring='roc_auc')\n",
    "    rs.fit(X_train, y_train)\n",
    "    return rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit tuned model\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "def fit_tuned_rf_model(model):\n",
    "    print(\"tuned hyperparameters :(best parameters) \", model.best_params_)\n",
    "    print(\"ROC AUC :\", model.best_score_)\n",
    "\n",
    "    modelCV=RandomForestClassifier(max_features=model.best_params_['max_features'], max_depth=model.best_params_['max_depth'], min_samples_split=model.best_params_['min_samples_split'], min_samples_leaf=model.best_params_['min_samples_leaf'], bootstrap=model.best_params_['bootstrap'],n_estimators=model.best_params_['n_estimators'])\n",
    "    modelCV.fit(X_train, y_train)\n",
    "\n",
    "    #return predicted probabilities\n",
    "    probs = modelCV.predict_proba(X_test)\n",
    "        # keep probabilities for the positive outcome only\n",
    "    probs = probs[:, 1]\n",
    "    roc_auc=roc_auc_score(y_test, probs)\n",
    "    fpr, tpr, _ = roc_curve(y_test, probs)\n",
    "    print(\"Fitted tuned random forest model \", roc_auc)\n",
    "    return modelCV, roc_auc, fpr, tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save labels: ranking<50=1, 0 otherwise\n",
    "y=pd.cut(df.Rank,bins=[0,50,100],labels=[1,0])\n",
    "df['y']=y\n",
    "df[['y','Rank']].groupby('y').mean()\n",
    "len(y)\n",
    "df.Rank.groupby(y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random forest classifier on tfidf features\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf,y, test_size=0.33, random_state=42)\n",
    "rf=rf_rs(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the tuned model to disk\n",
    "filename = 'tuned_rf_model.sav'\n",
    "pickle.dump(rf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit tuned model\n",
    "rf_fit=fit_tuned_rf_model(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot most important features\n",
    "importance=rf_fit[0].feature_importances_\n",
    "feat_dict=dict(zip(filtered_voc,importance))\n",
    "sort={k: v for k, v in sorted(feat_dict.items(), reverse=True, key=lambda item: item[1])}\n",
    "n_items=list(islice(sort.items(), 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs=[round(i[1],5) for i in n_items]\n",
    "indices=[i[0] for i in n_items]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "plt.barh(indices, coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select top 1000 features\n",
    "top_1000=list(sort.items())[:1000]\n",
    "top_1000_feat=[i[0] for i in top_1000]\n",
    "#select top feat from X_tfidf\n",
    "#get indices of top1000 features from filtered_voc\n",
    "top_1000_indices = [i for i, value in enumerate(filtered_voc) if value in top_1000_feat]\n",
    "X_1000=X_tfidf[:,top_1000_indices]\n",
    "X_1000.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Join music features, lyrics features and tfidf features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_features=['danceability', 'key', 'loudness','energy','acousticness', 'speechiness', 'mode', 'instrumentalness', 'liveness','valence', 'tempo']\n",
    "X_music=df[music_features]\n",
    "X_music=X_music.fillna(X_music.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of lyrics features\n",
    "\n",
    "add_features=['word_length', 'unique_words', 'total_words','count_rhyming_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join lyrics data with musical features\n",
    "#stack sparse matrices\n",
    "from scipy.sparse import hstack\n",
    "import scipy.sparse\n",
    "\n",
    "X_m=scipy.sparse.csr_matrix(X_music.values)\n",
    "print(X_m.shape)\n",
    "\n",
    "X_l=scipy.sparse.csr_matrix(df[add_features].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_1000.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#array of column names\n",
    "columns=filtered_voc+music_features+add_features\n",
    "col_names = np.array(columns)\n",
    "col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text=hstack([X_1000, X_l])\n",
    "X_all=hstack([X_text,X_m])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define parameter grid for logistic cv\n",
    "\n",
    "C = [0.00001,0.0001, 0.001, 0.01, 0.1,1,10,100,1000, 10000]\n",
    "penalty = ['l2']\n",
    "\n",
    "parameters = dict(C=C, penalty=penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_cv(X_train, X_test, y_train, y_test):\n",
    "    logistic=LogisticRegression(max_iter=500)\n",
    "    gsl=GridSearchCV(logistic, parameters, cv=3, n_jobs=-1, scoring=\"roc_auc\")\n",
    "    gsl.fit(X_train, y_train)\n",
    "    return gsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit tuned model\n",
    "def fit_tuned_lr_model(model):    \n",
    "    logisticCV=LogisticRegression(C=model.best_params_['C'], penalty=model.best_params_['penalty'], max_iter=500)\n",
    "    logisticCV.fit(X_train, y_train)\n",
    "\n",
    "    #return predicted probabilities\n",
    "    lr_probs = logisticCV.predict_proba(X_test)\n",
    "\n",
    "    # keep probabilities for the positive outcome only\n",
    "    lr_probs = lr_probs[:, 1]\n",
    "    roc_auc=roc_auc_score(y_test, lr_probs)\n",
    "    fpr, tpr, _ = roc_curve(y_test, lr_probs)\n",
    "    print(\"tuned hyperparameters :(best parameters) \", model.best_params_)\n",
    "    print(\"ROC AUC :\", model.best_score_)\n",
    "    print(\"Fitted tuned logit  model \", roc_auc)\n",
    "    return logisticCV,roc_auc, fpr, tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Classifier on tfidf features \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf,y, test_size=0.33, random_state=42)\n",
    "lr=logistic_cv(X_train, X_test, y_train, y_test)\n",
    "#fit tuned model\n",
    "lr_fit=fit_tuned_lr_model(lr)\n",
    "# save the tuned model to disk\n",
    "filename = 'tuned_logistic_tfidf.sav'\n",
    "pickle.dump(lr, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Classifier on tfidf top 1000 features only \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_1000,y, test_size=0.33, random_state=42)\n",
    "lr=logistic_cv(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#fit tuned model\n",
    "lr_fit_1000=fit_tuned_lr_model(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the tuned model to disk\n",
    "filename = 'tuned_logistic_1000.sav'\n",
    "pickle.dump(lr, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Classifier on tfidf top 1000 plus engineered text features  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_text,y, test_size=0.33, random_state=42)\n",
    "lr=logistic_cv(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the tuned model to disk\n",
    "filename = 'tuned_logistic_text.sav'\n",
    "pickle.dump(lr, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit tuned model\n",
    "lr_fit_text=fit_tuned_lr_model(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Classifier on tfidf top 1000 plus engineered text features plus music features\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all,y, test_size=0.33, random_state=42)\n",
    "lr=logistic_cv(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit tuned model\n",
    "lr_fit_all=fit_tuned_lr_model(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random forest classifier on tfidf and text features\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_text,y, test_size=0.33, random_state=42)\n",
    "rf=rf_rs(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the tuned model to disk\n",
    "filename = 'tuned_rf_only_text.sav'\n",
    "pickle.dump(rf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit tuned model\n",
    "rf_fit_text=fit_tuned_rf_model(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random forest classifier on all 1000 tfidf plus text and musical features\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all,y, test_size=0.33, random_state=42)\n",
    "rf=rf_rs(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the tuned model to disk\n",
    "filename = 'tuned_rf_all.sav'\n",
    "pickle.dump(rf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit tuned model\n",
    "rf_fit_all=fit_tuned_rf_model(rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#support vector mechines (SVM)\n",
    "from sklearn.svm import SVC\n",
    "#initiate model\n",
    "svm = SVC(probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tune hyperparameters with GridSearchCV\n",
    "# defining parameter range\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all,y, test_size=0.33, random_state=42)\n",
    "\n",
    "param_grid = {'C': [0.1, 1, 10, 100, 1000], \n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "              'kernel': ['rbf']} \n",
    "  \n",
    "svm_cv = GridSearchCV(svm, param_grid, cv=3, n_jobs=-1, scoring=\"roc_auc\")\n",
    "  \n",
    "# fitting the model for grid search\n",
    "svm_cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Tuned SVM Params: {}\".format(svm_cv.best_params_)) \n",
    "print(\"Best score is {}\".format(svm_cv.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make model with tuned parameters\n",
    "svm_tuned=SVC(C=svm_cv.best_params_['C'], gamma= svm_cv.best_params_['gamma'], kernel= svm_cv.best_params_['kernel'], probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_probs = svm_tuned.predict_proba(X_test)\n",
    "\n",
    "# keep probabilities for the positive outcome only\n",
    "svm_probs = svm_probs[:, 1]\n",
    "roc_auc=roc_auc_score(y_test, svm_probs)\n",
    "fpr, tpr, _ = roc_curve(y_test, svm_probs)\n",
    "\n",
    "print(\"Fitted tuned svm model \", roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drawing roc curve and auc\n",
    "\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# generate a no skill prediction (majority class)\n",
    "ns_probs = [0 for _ in range(len(y_test))]\n",
    "\n",
    "## plot the roc curve: start with no skill prediction\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "ns_auc = roc_auc_score(y_test, ns_probs) #no skill\n",
    "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
    "plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "\n",
    "\n",
    "#draw roc curves for tuned models\n",
    "def draw_roc_curve(model, label):\n",
    "    model_name=type(model[0]).__name__\n",
    "    # retrieve roc auc scores\n",
    "    auc = model[1]\n",
    "    # calculate roc curves\n",
    "    # get tpr, fpr\n",
    "    fpr=model[2]\n",
    "    tpr=model[3]\n",
    "    print('Model=%s; Data=%s; ROC AUC=%.3f' % (model_name, label, auc)) \n",
    "    lab=model_name+\" \"+label\n",
    "    plt.plot(fpr, tpr, marker='.', label=lab)\n",
    "\n",
    "    \n",
    "model_dict={'all tfidf features':lr_fit_tfidf, 'top 1000 tfidf features': lr_fit_1000, 'top tfidf plus text features': lr_fit_text,'all features': lr_fit_all, 'top 1000 tfidf features': rf_fit, 'top tfidf plus text features': rf_fit_text,'all features': rf_fit_all}\n",
    "\n",
    "\n",
    "#WHY CAN I NOT DRAW ALL LINES IN ONE GRAPH USING A LOOP?\n",
    "for k,v in model_dict.items():\n",
    "    draw_roc_curve(v,k)\n",
    "\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
